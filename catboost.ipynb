{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from catboost import CatBoostClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/Twitter_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shape with NaN values: {data.shape}\")\n",
    "data = data.dropna()\n",
    "data.shape\n",
    "print(f\"shape without NaN values: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\"-1\", \"0\", \"1\"]\n",
    "number_of_reviews = [data['category'].value_counts()[x] for x in range(-1, 2)]\n",
    "\n",
    "#define Seaborn color palette to use\n",
    "colors = sns.light_palette('seagreen')[0:3]\n",
    "\n",
    "#create pie chart\n",
    "plt.pie(number_of_reviews, labels = reviews, colors = colors, autopct='%.0f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_punctuation_from_string(s: str):\n",
    "    return re.sub(r'[^\\w\\s]','', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, \"clean_text\"] = data[\"clean_text\"].astype(str)\n",
    "data.loc[:, \"clean_text\"] = data[\"clean_text\"].apply(delete_punctuation_from_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, random_state=239)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer()\n",
    "\n",
    "x_train = bow.fit_transform(train[\"clean_text\"])\n",
    "x_test = bow.transform(test[\"clean_text\"])\n",
    "y_train = train[\"category\"]\n",
    "    \n",
    "model = CatBoostClassifier(verbose=0, task_type=\"GPU\")\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_test = test[\"category\"]\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "balanced_accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(x):\n",
    "    x = map(lambda r:  ' '.join([l.lemmatize(i.lower()) for i in r.split()]), x)\n",
    "    x = np.array(list(x))\n",
    "    return x\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def delete_stop_word(s):\n",
    "    words = s.split()\n",
    "    return \" \".join([word for word in words if word.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply lemmatization\n",
    "\n",
    "train[\"clean_text\"] = lemmatize(train[\"clean_text\"])\n",
    "test[\"clean_text\"] = lemmatize(test[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete stop words\n",
    "\n",
    "train[\"clean_text\"] = train[\"clean_text\"].apply(delete_stop_word)\n",
    "test[\"clean_text\"] = test[\"clean_text\"].apply(delete_stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose hyperparameters (TODO)\n",
    "\n",
    "parameters = {\"depth\": [5],\n",
    "          \"iterations\": [500],\n",
    "          \"learning_rate\": [0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer()\n",
    "\n",
    "x_train = bow.fit_transform(train[\"clean_text\"])\n",
    "x_test = bow.transform(test[\"clean_text\"])\n",
    "y_train = train[\"category\"]\n",
    "\n",
    "# clf = GridSearchCV(CatBoostClassifier(verbose=0), parameters)\n",
    "clf = CatBoostClassifier(iterations=500, depth=5, learning_rate=0.1, verbose=0)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_test = test[\"category\"]\n",
    "\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_pred, y_test) # parameters are bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\"AMAZING I LOVE YOU ALL THERE\", \"I used your service once, but it was terrible\"]\n",
    "\n",
    "reviews = lemmatize(reviews)\n",
    "reviews = bow.transform(reviews)\n",
    "\n",
    "clf.predict(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE CATBOOST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/model_catboost\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m model_file:\n\u001b[1;32m----> 4\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump((\u001b[43mmodel\u001b[49m, bow), model_file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"models/model_catboost\", 'wb') as model_file:\n",
    "    pickle.dump((model, bow), model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactor import Interactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelsObj = Interactor(models_path=\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelsObj.predict(model_name=\"catboost\", sentence=\"AMAZING I LOVE YOU ALL THERE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
